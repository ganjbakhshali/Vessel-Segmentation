{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_x=sorted(glob(os.path.join(path,\"training/images\",\"*.tif\")))\n",
    "    train_y=sorted(glob(os.path.join(path,\"training/1st_manual\",\"*.gif\")))\n",
    "    \n",
    "    test_x=sorted(glob(os.path.join(path,\"test/images\",\"*.tif\")))\n",
    "    test_y=sorted(glob(os.path.join(path,\"test/1st_manual\",\"*.gif\")))\n",
    "\n",
    "    return (train_x,train_y) , (test_x,test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(images,masks,save_path, augment=True):\n",
    "    size=(512,512)\n",
    "\n",
    "\n",
    "    for idx , (x,y) in tqdm(enumerate(zip(images,masks)), total=len(images)):\n",
    "\n",
    "        # extracting name of data and mask\n",
    "\n",
    "        xname=x.split(\"/\")[-1].split(\".\")[0]\n",
    "        \n",
    "\n",
    "        # reading iamge and its mask:\n",
    "\n",
    "        x = cv2.imread(x,cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0] \n",
    "        # print(x.shape, y.shape)\n",
    "\n",
    "\n",
    "        if augment:\n",
    "            \n",
    "            aug=HorizontalFlip(p=1.0)\n",
    "            augmented=aug(image=x,mask=y)\n",
    "            x1=augmented[\"image\"]\n",
    "            y1=augmented[\"mask\"]\n",
    "\n",
    "            aug=VerticalFlip(p=1.0)\n",
    "            augmented=aug(image=x,mask=y)\n",
    "            x2=augmented[\"image\"]\n",
    "            y2=augmented[\"mask\"]\n",
    "\n",
    "            aug=Rotate(limit=45, p=1.0)\n",
    "            augmented=aug(image=x,mask=y)\n",
    "            x3=augmented[\"image\"]\n",
    "            y3=augmented[\"mask\"]\n",
    "\n",
    "\n",
    "            X = [x, x1,x2,x3]\n",
    "            Y =[y,y1,y2,y3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            X=[x]\n",
    "            Y=[y]\n",
    "\n",
    "        \n",
    "        # save images:\n",
    "        index=0\n",
    "        for i , m in zip(X,Y):\n",
    "            i=cv2.resize(i,size)\n",
    "            m=cv2.resize(m,size)\n",
    "\n",
    "            tmp_image_name=f\"{xname}_{index}.png\" \n",
    "            tmp_mask_name= f\"{xname}_{index}.png\" \n",
    "\n",
    "            image_path= os.path.join(save_path,\"image\",tmp_image_name)\n",
    "            mask_path = os.path.join(save_path,\"mask\",tmp_mask_name)\n",
    "\n",
    "            cv2.imwrite(image_path,i)\n",
    "            cv2.imwrite(mask_path,m)\n",
    "\n",
    "            index+=1\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: data:20 - mask:20\n",
      "Test: data:20 - mask:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 28.16it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 88.74it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    # seeding\n",
    "\n",
    "    np.random.seed(42)\n",
    "    data_path=\"RetinaData\"\n",
    "    (train_x,train_y) , (test_x,test_y)= load_data(data_path)\n",
    "\n",
    "    print(f\"Train: data:{len(train_x)} - mask:{len(train_y)}\")\n",
    "    print(f\"Test: data:{len(test_x)} - mask:{len(test_y)}\")\n",
    "\n",
    "\n",
    "    # create Dirs for Augment\n",
    "    create_dir(\"RetinaData/new_data/train/image\")\n",
    "    create_dir(\"RetinaData/new_data/train/mask\")\n",
    "\n",
    "    create_dir(\"RetinaData/new_data/test/image\")\n",
    "    create_dir(\"RetinaData/new_data/test/mask\")\n",
    "\n",
    "\n",
    "    #data Aug\n",
    "    augment_data(train_x,train_y, \"RetinaData/new_data/train\",augment=True)\n",
    "    augment_data(test_x,test_y, \"RetinaData/new_data/test\",augment=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1= nn.Conv2d(in_c,out_c, kernel_size=3, padding=1)\n",
    "        self.bn1= nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2= nn.Conv2d(out_c,out_c, kernel_size=3, padding=1)\n",
    "        self.bn2= nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        x=self.conv1(inputs)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c,out_c ):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c,out_c)\n",
    "        self.pool=nn.MaxPool2d((2,2))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x=self.conv(inputs)\n",
    "        p= self.pool(x)\n",
    "\n",
    "        return x,p\n",
    "\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c,out_c ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(in_c,out_c,kernel_size=2,stride=2, padding=0)\n",
    "        self.conv=conv_block(out_c+out_c,out_c)\n",
    "\n",
    "\n",
    "    def forward(self, inputs,skip):\n",
    "        x= self.up(inputs)\n",
    "        x=torch.cat([x,skip],axis=1) #concat on channels\n",
    "        x=self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.e1=encoder_block(3,64)\n",
    "        self.e2=encoder_block(64,128)\n",
    "        self.e3=encoder_block(128,256)\n",
    "        self.e4=encoder_block(256,512)\n",
    "\n",
    "        #  Bottelneck\n",
    "\n",
    "        self.b= conv_block(512,1024)\n",
    "\n",
    "        #decoder\n",
    "        self.d1=decoder_block(1024,512)\n",
    "        self.d2=decoder_block(512,256)\n",
    "        self.d3=decoder_block(256,128)\n",
    "        self.d4=decoder_block(128,64)\n",
    "\n",
    "        # classifier\n",
    "        self.output= nn.Conv2d(64,1,kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        # encoder\n",
    "\n",
    "        s1,p1=self.e1(inputs)\n",
    "        s2,p2=self.e2(p1)\n",
    "        s3,p3=self.e3(p2)\n",
    "        s4,p4=self.e4(p3)\n",
    "\n",
    "\n",
    "        #Bottelneck\n",
    "        b=self.b(p4)\n",
    "\n",
    "        #decoder\n",
    "\n",
    "        d1=self.d1(b,s4)\n",
    "        d2=self.d2(d1,s3)\n",
    "        d3=self.d3(d2,s2)\n",
    "        d4=self.d4(d3,s1)\n",
    "\n",
    "        output=self.output(d4)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "#test bulding model\n",
    "\n",
    "x= torch.randn((2,3,512,512))\n",
    "f=build_unet()\n",
    "\n",
    "y=f(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\"\"\" Seeding the randomness. \"\"\"\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\"\"\" Create a directory. \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Reading image\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image/255.0 ## (512, 512, 3)\n",
    "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        # Reading mask \n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask/255.0   ## (512, 512)\n",
    "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aria/anaconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss improved from inf to 1.3307. Saving checkpoint: files/ch.pth\n",
      "Epoch: 01 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 1.164\n",
      "\t Val. Loss: 1.331\n",
      "\n",
      "Valid loss improved from 1.3307 to 0.9809. Saving checkpoint: files/ch.pth\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.980\n",
      "\t Val. Loss: 0.981\n",
      "\n",
      "Valid loss improved from 0.9809 to 0.8900. Saving checkpoint: files/ch.pth\n",
      "Epoch: 03 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.921\n",
      "\t Val. Loss: 0.890\n",
      "\n",
      "Valid loss improved from 0.8900 to 0.8511. Saving checkpoint: files/ch.pth\n",
      "Epoch: 04 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.875\n",
      "\t Val. Loss: 0.851\n",
      "\n",
      "Valid loss improved from 0.8511 to 0.8118. Saving checkpoint: files/ch.pth\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.835\n",
      "\t Val. Loss: 0.812\n",
      "\n",
      "Valid loss improved from 0.8118 to 0.7723. Saving checkpoint: files/ch.pth\n",
      "Epoch: 06 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.798\n",
      "\t Val. Loss: 0.772\n",
      "\n",
      "Valid loss improved from 0.7723 to 0.7495. Saving checkpoint: files/ch.pth\n",
      "Epoch: 07 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.761\n",
      "\t Val. Loss: 0.749\n",
      "\n",
      "Valid loss improved from 0.7495 to 0.7196. Saving checkpoint: files/ch.pth\n",
      "Epoch: 08 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.727\n",
      "\t Val. Loss: 0.720\n",
      "\n",
      "Valid loss improved from 0.7196 to 0.6864. Saving checkpoint: files/ch.pth\n",
      "Epoch: 09 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.698\n",
      "\t Val. Loss: 0.686\n",
      "\n",
      "Valid loss improved from 0.6864 to 0.6600. Saving checkpoint: files/ch.pth\n",
      "Epoch: 10 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.668\n",
      "\t Val. Loss: 0.660\n",
      "\n",
      "Valid loss improved from 0.6600 to 0.6334. Saving checkpoint: files/ch.pth\n",
      "Epoch: 11 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.633\n",
      "\n",
      "Valid loss improved from 0.6334 to 0.6110. Saving checkpoint: files/ch.pth\n",
      "Epoch: 12 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.614\n",
      "\t Val. Loss: 0.611\n",
      "\n",
      "Valid loss improved from 0.6110 to 0.5852. Saving checkpoint: files/ch.pth\n",
      "Epoch: 13 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.591\n",
      "\t Val. Loss: 0.585\n",
      "\n",
      "Valid loss improved from 0.5852 to 0.5732. Saving checkpoint: files/ch.pth\n",
      "Epoch: 14 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.568\n",
      "\t Val. Loss: 0.573\n",
      "\n",
      "Valid loss improved from 0.5732 to 0.5518. Saving checkpoint: files/ch.pth\n",
      "Epoch: 15 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.546\n",
      "\t Val. Loss: 0.552\n",
      "\n",
      "Valid loss improved from 0.5518 to 0.5326. Saving checkpoint: files/ch.pth\n",
      "Epoch: 16 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.527\n",
      "\t Val. Loss: 0.533\n",
      "\n",
      "Valid loss improved from 0.5326 to 0.5106. Saving checkpoint: files/ch.pth\n",
      "Epoch: 17 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.509\n",
      "\t Val. Loss: 0.511\n",
      "\n",
      "Valid loss improved from 0.5106 to 0.5054. Saving checkpoint: files/ch.pth\n",
      "Epoch: 18 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.493\n",
      "\t Val. Loss: 0.505\n",
      "\n",
      "Valid loss improved from 0.5054 to 0.4954. Saving checkpoint: files/ch.pth\n",
      "Epoch: 19 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.478\n",
      "\t Val. Loss: 0.495\n",
      "\n",
      "Valid loss improved from 0.4954 to 0.4756. Saving checkpoint: files/ch.pth\n",
      "Epoch: 20 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.462\n",
      "\t Val. Loss: 0.476\n",
      "\n",
      "Valid loss improved from 0.4756 to 0.4683. Saving checkpoint: files/ch.pth\n",
      "Epoch: 21 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.446\n",
      "\t Val. Loss: 0.468\n",
      "\n",
      "Valid loss improved from 0.4683 to 0.4577. Saving checkpoint: files/ch.pth\n",
      "Epoch: 22 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.432\n",
      "\t Val. Loss: 0.458\n",
      "\n",
      "Valid loss improved from 0.4577 to 0.4538. Saving checkpoint: files/ch.pth\n",
      "Epoch: 23 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.420\n",
      "\t Val. Loss: 0.454\n",
      "\n",
      "Valid loss improved from 0.4538 to 0.4481. Saving checkpoint: files/ch.pth\n",
      "Epoch: 24 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.407\n",
      "\t Val. Loss: 0.448\n",
      "\n",
      "Valid loss improved from 0.4481 to 0.4351. Saving checkpoint: files/ch.pth\n",
      "Epoch: 25 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.394\n",
      "\t Val. Loss: 0.435\n",
      "\n",
      "Valid loss improved from 0.4351 to 0.4313. Saving checkpoint: files/ch.pth\n",
      "Epoch: 26 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.382\n",
      "\t Val. Loss: 0.431\n",
      "\n",
      "Valid loss improved from 0.4313 to 0.4303. Saving checkpoint: files/ch.pth\n",
      "Epoch: 27 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.371\n",
      "\t Val. Loss: 0.430\n",
      "\n",
      "Valid loss improved from 0.4303 to 0.4213. Saving checkpoint: files/ch.pth\n",
      "Epoch: 28 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.360\n",
      "\t Val. Loss: 0.421\n",
      "\n",
      "Valid loss improved from 0.4213 to 0.4167. Saving checkpoint: files/ch.pth\n",
      "Epoch: 29 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.345\n",
      "\t Val. Loss: 0.417\n",
      "\n",
      "Epoch: 30 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.335\n",
      "\t Val. Loss: 0.420\n",
      "\n",
      "Epoch: 31 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.325\n",
      "\t Val. Loss: 0.421\n",
      "\n",
      "Valid loss improved from 0.4167 to 0.4084. Saving checkpoint: files/ch.pth\n",
      "Epoch: 32 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.313\n",
      "\t Val. Loss: 0.408\n",
      "\n",
      "Epoch: 33 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.304\n",
      "\t Val. Loss: 0.410\n",
      "\n",
      "Epoch: 34 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.295\n",
      "\t Val. Loss: 0.419\n",
      "\n",
      "Valid loss improved from 0.4084 to 0.3989. Saving checkpoint: files/ch.pth\n",
      "Epoch: 35 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.288\n",
      "\t Val. Loss: 0.399\n",
      "\n",
      "Epoch: 36 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.280\n",
      "\t Val. Loss: 0.401\n",
      "\n",
      "Valid loss improved from 0.3989 to 0.3970. Saving checkpoint: files/ch.pth\n",
      "Epoch: 37 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.272\n",
      "\t Val. Loss: 0.397\n",
      "\n",
      "Valid loss improved from 0.3970 to 0.3929. Saving checkpoint: files/ch.pth\n",
      "Epoch: 38 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.268\n",
      "\t Val. Loss: 0.393\n",
      "\n",
      "Epoch: 39 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.261\n",
      "\t Val. Loss: 0.395\n",
      "\n",
      "Epoch: 40 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.255\n",
      "\t Val. Loss: 0.393\n",
      "\n",
      "Epoch: 41 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.248\n",
      "\t Val. Loss: 0.398\n",
      "\n",
      "Epoch: 42 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.243\n",
      "\t Val. Loss: 0.394\n",
      "\n",
      "Valid loss improved from 0.3929 to 0.3895. Saving checkpoint: files/ch.pth\n",
      "Epoch: 43 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.238\n",
      "\t Val. Loss: 0.390\n",
      "\n",
      "Valid loss improved from 0.3895 to 0.3890. Saving checkpoint: files/ch.pth\n",
      "Epoch: 44 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.237\n",
      "\t Val. Loss: 0.389\n",
      "\n",
      "Epoch: 45 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.232\n",
      "\t Val. Loss: 0.389\n",
      "\n",
      "Valid loss improved from 0.3890 to 0.3868. Saving checkpoint: files/ch.pth\n",
      "Epoch: 46 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.229\n",
      "\t Val. Loss: 0.387\n",
      "\n",
      "Epoch: 47 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.224\n",
      "\t Val. Loss: 0.390\n",
      "\n",
      "Valid loss improved from 0.3868 to 0.3863. Saving checkpoint: files/ch.pth\n",
      "Epoch: 48 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.221\n",
      "\t Val. Loss: 0.386\n",
      "\n",
      "Epoch: 49 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.217\n",
      "\t Val. Loss: 0.388\n",
      "\n",
      "Valid loss improved from 0.3863 to 0.3849. Saving checkpoint: files/ch.pth\n",
      "Epoch: 50 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.216\n",
      "\t Val. Loss: 0.385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "seeding(42)\n",
    "\n",
    "# create dir for checkpoints\n",
    "create_dir(\"files\")\n",
    "\n",
    "#load dataset\n",
    "train_x=sorted(glob(\"RetinaData/new_data/train/image/*\"))\n",
    "train_y=sorted(glob(\"RetinaData/new_data/train/mask/*\"))\n",
    "\n",
    "test_x=sorted(glob(\"RetinaData/new_data/test/image/*\"))\n",
    "test_y=sorted(glob(\"RetinaData/new_data/test/mask/*\"))\n",
    "\n",
    "# print(f\"data size: train_x:{len(train_x)} - test_x:{len(test_x)}\")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "H=512\n",
    "W=512\n",
    "\n",
    "size=(H,W)\n",
    "\n",
    "batch_size=2\n",
    "epochs=50\n",
    "lr=1e-4\n",
    "checkpoint_path=\"files/ch.pth\"\n",
    "\n",
    "\n",
    "# load dataset\n",
    "\"\"\" Dataset and loader \"\"\"\n",
    "train_dataset = DriveDataset(train_x, train_y)\n",
    "test_dataset = DriveDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "device = torch.device('cuda')   ## RTX 3060\n",
    "model = build_unet()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "loss_fn = DiceBCELoss()\n",
    "\n",
    "\"\"\" Training the model \"\"\"\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    valid_loss = evaluate(model, test_loader, loss_fn, device)\n",
    "\n",
    "    \"\"\" Saving the model \"\"\"\n",
    "    if valid_loss < best_valid_loss:\n",
    "        data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "        print(data_str)\n",
    "\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "    data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "    print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49773/1326212625.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard: 0.6601 - F1: 0.7949 - Recall: 0.7733 - Precision: 0.8233 - Acc: 0.9654\n",
      "FPS:  775.5667939460618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "from operator import add\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    #Ground truth\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_true = y_true > 0.5\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_true = y_true.reshape(-1)\n",
    "\n",
    "    #Prediction \n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    score_jaccard = jaccard_score(y_true, y_pred)\n",
    "    score_f1 = f1_score(y_true, y_pred)\n",
    "    score_recall = recall_score(y_true, y_pred)\n",
    "    score_precision = precision_score(y_true, y_pred)\n",
    "    score_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "\n",
    "checkpoint_path = \"files/ch.pth\"\n",
    "create_dir(\"results\")\n",
    "#Load the checkpoint\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = build_unet()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "time_taken = []\n",
    "\n",
    "for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
    "    #Extract the name\n",
    "    name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    #Reading image\n",
    "    image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
    "    ## image = cv2.resize(image, size)\n",
    "    x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
    "    x = x/255.0\n",
    "    x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
    "    x = x.astype(np.float32)\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.to(device)\n",
    "\n",
    "    #Reading mask\n",
    "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "    ## mask = cv2.resize(mask, size)\n",
    "    y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
    "    y = y/255.0\n",
    "    y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
    "    y = y.astype(np.float32)\n",
    "    y = torch.from_numpy(y)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #Prediction and Calculating FPS\n",
    "        start_time = time.time()\n",
    "        pred_y = model(x)\n",
    "        pred_y = torch.sigmoid(pred_y)\n",
    "        total_time = time.time() - start_time\n",
    "        time_taken.append(total_time)\n",
    "\n",
    "\n",
    "        score = calculate_metrics(y, pred_y)\n",
    "        metrics_score = list(map(add, metrics_score, score))\n",
    "        pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
    "        pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
    "        pred_y = pred_y > 0.5\n",
    "        pred_y = np.array(pred_y, dtype=np.uint8)\n",
    "\n",
    "    #Saving masks\n",
    "    ori_mask = mask_parse(mask)\n",
    "    pred_y = mask_parse(pred_y)\n",
    "    line = np.ones((size[1], 10, 3)) * 128\n",
    "\n",
    "    cat_images = np.concatenate(\n",
    "        [image, line, ori_mask, line, pred_y * 255], axis=1\n",
    "    )\n",
    "    cv2.imwrite(f\"results/{name}.png\", cat_images)\n",
    "\n",
    "jaccard = metrics_score[0]/len(test_x)\n",
    "f1 = metrics_score[1]/len(test_x)\n",
    "recall = metrics_score[2]/len(test_x)\n",
    "precision = metrics_score[3]/len(test_x)\n",
    "acc = metrics_score[4]/len(test_x)\n",
    "print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}\")\n",
    "\n",
    "fps = 1/np.mean(time_taken)\n",
    "print(\"FPS: \", fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
